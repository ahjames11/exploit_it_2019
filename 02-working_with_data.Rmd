# Working with Data {#data}

All data analysis projects follow a series of steps. Practitioners across a wide variety of disciplines may argue the specifics, but a general workflow involves:

1.  Research question formulation
2.  Data collection
3.  Data manipulation and cleaning (wrangling)
4.  Exploratory data analysis
5.  Formal analysis

This workflow can become very cyclical in nature. A specific research question may guide data collection, cleaning, and analysis. If the analysis yields unexpected results, then the process is restarted – hopefully reusing some of the original data.

It is very difficult to perfectly execute each step. However, a well-executed data collection followed by proper manipulation and cleaning creates a strong foundation for both exploratory and formal analyses. This foundational dataset should be able to support multiple research questions, including ones of the future.

The remainder of  Chapter \@ref(data) discusses best practices for storing, structuring, and manipulating data. This is not an exhaustive list; it takes experience to fully grasp data manipulation best practices.

## Storing and Structuring Data

It all starts with the data structure. More often than not, if you find yourself spending a lot of time reorganizing data before performing a task (e.g., summarizing data, creating a graphic) it is because your data are inefficiently structured. While some post processing will always be required, simple techniques exist to greatly ease data manipulation down the road.

> Data should be convenient for the computer to work with. This is not necessarily natural and easy for a human to read! This is a critical distinction. Data are **stored** to be easy to use and manipulate. Data are **reported** to be easy to consume by a human – be it graphical, tabular, or another form.

### Rows and Columns

It sounds obvious at first, but structured data are best organized in a tabular format. Think of the table as a collection of rows and columns.

**Rows** represent unique data observations. Each data row must be consistently defined. Deviation from this definition across rows must be avoided.

**Columns** represent variables. Variables are either hard coded data or calculations based on other variables. The consistent definition across all rows within a column allows for the use of vector operations. Vector operations require some practice at first, but are simpler (and more computationally efficient) in the long run.

Consider the following data structure in Table 1. This represents a reporting format, not a storage structure.

1.  There is white space within the table in the form of blank cells. While easy to read, this creates problems recognizing data entries from a computer’s perspective. This includes both blank rows and spacing in the System column.
2.  Column definitions are inconsistent. Sometimes this is unavoidable, but often there are better ways to handle it. In this example we are mixing three ideas. There is cost data (Engine, Remaining, PM), quantity data (Number of Units), and then a calculation ($Cost \times Units$) for Total Cost.

TABLE1 TABLE2

Table 2 shows a much improved structure. At a quick glance this actually appears more difficult to read. The summary Total Cost is gone, which some may find to be valuable information. The repetition in System adds more text which can be distracting. However, this structure is much more convenient to work with computationally.

For larger applications, the structure shown across Table 3 and Table 4 may be even better. Duplication has now been removed. Row and column definitions are clear and consistent. Adding (appending) additional data is easy and the data tables are easily to manipulate to create a more desirable reporting table or graphic.

TABLE3 TABLE4

### Single Purpose Variables

Always create single purpose variables. A prime example of this is within a Work Breakdown Structure (WBS). It is very common to see a WBS with Model incorporated into the structure. In this case, the WBS now represents both a Model and a Cost Element. For example:

1	Surface Vehicle System
  1.1	Variant A
    1.1.1	Surface Vehicle
      1.1.1.1	Engine
      1.1.1.2	Remaining Vehicle
  1.2	Variant B
    1.2.1	Surface Vehicle
      1.2.1.1	Engine
      1.2.1.2	Remaining Vehicle

This structure is useful for reporting, but makes analysis painful. For 1.2.1.1 Engine, we must trace up to its grandparent element, 1.2, to know that it belongs to Variant B. This is not an easy task to do repeatedly within an analysis. A much better structure would be to move the Model out of the WBS into a column tag, as shown in Table 5.

TABLE5

This is not intended as an instruction on how to develop a WBS, but instead a demonstration of a preprocessing step to structure the data more effectively for analysis.

### Other Data Tips

The following sections briefly overview some other topics to consider when structuring data.

#### Variable Names

Naming variables can be hard. Names should be descriptive, yet concise. Create variable names that:

- are unique (i.e., do not name two different columns the same thing)
- do not have special characters (except the underscore)
- do not have spaces
- start with a letter

Use underscores and/or CamelCase to string together multiple words. In general, use common sense and include a data dictionary or definition so that there are no questions as to what a specific variable represents.

TABLE6

#### Ordered Records

Do not ascribe meaning to the physical sequencing of data. Data in a table are unordered. Data rows should be able to be shuffled randomly without any loss of information.

Define a variable to store any information that may be required to preserve the ordering of a dataset. Simply sort on that variable should something happen to the data and you need to restore or utilize the ordering.

#### Formatting

There is an important distinction between a data value and a data format. The value is the true representation of the data. The formatting is simply what is being displayed. Table 7 shows a few common examples.

TABLE7

Best practice is to store unformatted data. Formatting is later applied during reporting. Excel’s usefulness with formatting varies from convenient to highly problematic. Conveniently, Excel allows a cell to store a value, but display formatting. For example, a cell value may be 37284.345, but Excel will display 37,284.3 on the screen. No information is lost, and Excel retains the true cell value for calculations. Problematically, Excel sometimes “guesses” wrong (such as formatting WBS element 1.10 to 1.1) and exports formatted values that are not easily recognized by other software. This is discussed in more detail in the following section.

#### Data Types

Be sure to understand your variable data types. There are three generic data types:

1.  numeric
2.  date
3.  text

Software developers break these down even further, but less formal applications are fine considering just these three. The following are brief notes on each data type.

1.  **Numeric** data are any type of number; be it a {0, 1} field to represent true and false, an integer, or a number with decimals. A formatted number such as 1,000 is not a numeric field. It looks like a number, but the comma character is text. This is an issue particularly when exporting data as CSV from Excel.
2.  **Date** data are actually a special case of numeric data. Computers store dates as the number of days from some start date (1/1/1900 for Excel). The start date is generally different for each software package. Therefore, the safest way to store a date is as a character string in the ISO date format: YYYY-MM-DD.^[https://www.iso.org/iso-8601-date-and-time-format.html] This is an internationally recognized standard that any software will be able to interpret with certainty.
3.  **Text** data are basically anything that is not a number. Any formatting of a numeric variable causes the data to be converted to text. Some software packages will automatically convert back to numeric for you, but most will not.

When in doubt, it is best to control data types on your own. In our cost analysis world, WBSs can be highly problematic. A WBS “number” is actually a text field. Most data analysis packages will handle this naturally by defining the entire variable as text. However, we must be careful in Excel.

FIGURE1

Figure 1 exemplifies what occurs in Excel when you allow it to handle formatting. Column A shows a WBS number with the Number Format set to Text. Column B contains the same values, but with the General format selected. Excel adheres to standard convention of left justifying text and right justifying numbers, by default.

Clearly, this automatic formatting is a problem. Excel recognizes the WBS to be numeric, with the exception of 1.1.1 and 1.1.2, which are treated as text. In particular, 1.10 is now 1.1, as it dropped the trailing zero.

This is actually more problematic than it appears. The naive solution is simply to format any WBS column as text. Unfortunately, this fails. A simple `SUMIF()`, for example, will match 1.1 to both 1.1 and 1.10, summing values incorrectly. The following are some ways to mitigate this issue:

- Prefix the WBS with a letter (e.g., X1.1, X1.1.1, etc.) to force the structure into text
- Fill in a complete WBS to the lowest level with zeros (e.g., 1.0.0, 1.1.0, 1.1.1)
- Parse the WBS into multiple numeric fields (e.g., 1.2.3 becomes three fields: {1, 2, 3})
- Do not use Excel

#### Redundant Information

Avoid storing redundant data. Redundant data can be calculated from other data fields. Below are a few examples:

1.  Only store the child element values of a WBS. The values for the parent tasks can easily be created by summing their children.
2.  Do not store subtotals and totals. These can also be calculated very easily. A common occurrence is non-recurring and recurring costs. There is no need to store total cost in this case.
3.  Do not store values that can be derived from a formula. There is no need to store density in a dataset that contains mass and volume, assuming the simple equation holds.

$$density = \frac{mass}{volume}$$

#### Intermediary Tables

Redundant information may actually be important and it is tedious and inefficient to constantly recreate it. Using example (3) from the prior section, your main variable of interest may be density. In this case, create intermediary tables as part of your workflow.


FIGURE2

Assuming properly structured data tables, simple transformations (such as those described in the next section) easily and automatically create the intermediary tables. Base your analysis on these representations rather than the raw data for a clean, logical, and maintainable workflow. This strategy naturally segregates data from analysis, making the data more useful for alternative use cases.

#### Sparse Data

Data are considered sparse if many of the records have values equal to zero. The representation in Table 8 is considered the dense representation. We can reduce the amount of information stored by assuming that any value not referenced is in fact a zero. 

TABLE8

The data in Table 9, Table 10, and Table 11 can be used to rebuild that in Table 8. While this does involve an extra step, creating these tables constitutes best practice in a database environment. Under this representation, we only need to store two versus 12 data records.  

TABLE9 TABLE10 TABLE11

Storing data under a sparse model is not always appropriate. But in certain cases, it yields powerful results by greatly reducing the total volume of data.

## Understanding a Grammar

Grammar, the rules to which we abide within a language, enables us to communicate in a structured, repeatable way – even if the meaning itself is nonsensical. While most known in the context of written and spoken language, grammars have become popular elsewhere. Two of the most well-known grammars in data science are the:

1.  Grammar of Graphics
2.  Grammar of Data Manipulation

> gram∙mar (noun)
>
> The whole system and structure of a language or of languages in general, usually taken as consisting of syntax and morphology (including inflections) and sometimes also phonology and semantics.
>
> -- OED Online, Oxford University Press

These two grammars have been implemented with great success by Hadley Wickham and RStudio in the extremely popular R packages `ggplot2` and `dplyr`.^[Hadley Wickham (https://github.com/hadley) is a Chief Scientist at RStudio, focusing on tool development to “make data science easier, faster, and more fun”. He is viewed as an expert and thought leader in the development of the open source software R, specifically in the areas of data manipulation and visualization.] The grammars provide a set of instructions, guided by a **noun** (i.e., a dataset) and a set of **verbs**. These concepts translate well to other languages and modeling tools, including Excel and Access. Section 4 demonstrates how we leveraged the Grammar of Data Manipulation to create a flexible, scalable framework to analyze Stryker cost data.

### Grammar of Data Manipulation

