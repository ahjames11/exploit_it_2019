# Introduction {#intro}

Cost analysts have historically operated on small data sets. Although identifying, collecting, and preparing the ‘best’ data for analysis can prove tedious and time consuming, the small volume of information made for a manageable effort to acquire, ready, and analyze the data. The combination of modern data management tools and an ever-increasing volume of centrally collected/stored cost (and to a lesser extent, technical, and programmatic) data represents a tremendous opportunity for the DoD cost analysis community and the various consumers of its analytical products.

This paper discusses an approach to normalize, categorize, and analyze data as applied to a large subset (1,000,000+ records) of the OSD Cost Assessment and Program Evaluation (CAPE) managed Contractor Cost Data Report (CCDR) dataset. The resulting product is a compact, usable database that can answer any question the data itself is capable of answering, without additional prep work. Example problems quickly answered (by virtue of a dynamic dashboard) include investigating labor rates over time, cost trends by WBS, and cost growth.

The methodologies presented are consistent with data science best practices – formalized by the popular “grammar of data manipulation” implemented in R through the collection of `tidyverse` packages. ^[R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/] ^[https://www.tidyverse.org/] The goal is to communicate a flexible, scalable thought process which can be applied to the remainder of the CCDR library, as well as any other dataset.

**Why are we doing this?**

The motivation for this paper is a project initiated in fall 2017 by the Project Manager Stryker Brigade Combat Team (PM-SBCT). The cost team had amassed a large collection of CCDRs and was looking for new ways to exploit the data. More specifically, the goal of this project was to develop a Stryker-specific CCDR database and analysis tool to assist in the advanced validation and benchmarking of future data submissions.

The CCDR dataset employed for this effort is voluminous and inconsistent. Past CCDR-oriented research typically involved a limited data subset to target a specific, isolated question. This approach resulted in a never ending cycle of data cleansing and rework each and every time.
In the interest of future efficiencies, this project operated on the entire Stryker dataset for the purpose of normalizing the data for any/all future analytical purposes. As part of this forward-looking non-recurring effort, the work also established a consistent framework and structure to streamline data maintenance and analysis. While the primary goal of the work described in this paper was analysis of Stryker data, the workflow and data strategies were developed for use with any subset of the CCDR data available to cost analysts today. As such, virtually all the work discussed herein can be easily adapted to another program (or programs).

**What’s wrong with Excel?**

Excel is often our tool of choice. Nearly all analysts are trained in its use and it is available on virtually all government workstations. While convenient, Excel has severe limitations. Working with data in it is actually quite difficult. It is very hard to produce repeatable workflows, and it struggles to keep up with even a moderately sized dataset. The software itself enforces no sense of consistency. As a result, Excel analyses tend to be “throw away” – built for a specific task but far too difficult to maintain and expand as a living solution.
What are the other options?

There are a variety of software tools on the market to work with data. An option growing rapidly in popularity is R, an open source project focused on statistical computing. For decades it lived as a niche product, but has exploded in popularity during the “data science” boom in the mid-2000s. R is able to handle larger sets of data much more easily than Excel. As a scripting language, it is easy to create repeatable, reproducible work.
This paper discusses best practices for working with data through the lens of R. However, the concepts are not specific to the software package itself. The topics discussed in this paper can be applied to any other software choice, including Excel.
