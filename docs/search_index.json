[
["index.html", "Don’t Just Use Your Data… Exploit It! Preface", " Don’t Just Use Your Data… Exploit It! Adam H. James 2019-11-01 Preface Cost analysts historically operate on small, manageable data sets. Preparing data for a task may be tedious and time consuming, but overall achievable. Those days are over. More and more data are being collected, yet its storage and collection are handled under the same strategies as yesteryear. The result is data sets and databases which are difficult, if not impossible, to work with at scale. This paper discusses an approach to normalize, categorize, and analyze data as applied to a large subset (~ 2,000,000 records) of the OSD CAPE’s CSDR data set. The resulting product is a compact, usable database that can answer any question the data itself is capable of answering, without additional prep work. Example problem sets quickly answered (by virtue of dynamic dashboard) include investigating labor rates over time, cost trends by WBS, and an assessment of cost growth. The methodologies presented are consistent with data science best practices – formalized by the popular “grammar of data manipulation” implemented in the R software and the related “tidyverse” packages. The goal is to communicate a flexible, scalable thought process which can be applied to the remainder of the CSDR library, as well as any other data set. This paper was presented at the 2019 International Cost Estimating &amp; Analysis Association (ICEAA) Professional Development &amp; Training Workshop where it received the Best Paper award in theAnalysis &amp; Modeling, Machine Learning category. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Cost analysts have historically operated on small data sets. Although identifying, collecting, and preparing the ‘best’ data for analysis can prove tedious and time consuming, the small volume of information made for a manageable effort to acquire, ready, and analyze the data. The combination of modern data management tools and an ever-increasing volume of centrally collected/stored cost (and to a lesser extent, technical, and programmatic) data represents a tremendous opportunity for the DoD cost analysis community and the various consumers of its analytical products. This paper discusses an approach to normalize, categorize, and analyze data as applied to a large subset (1,000,000+ records) of the OSD Cost Assessment and Program Evaluation (CAPE) managed Contractor Cost Data Report (CCDR) dataset. The resulting product is a compact, usable database that can answer any question the data itself is capable of answering, without additional prep work. Example problems quickly answered (by virtue of a dynamic dashboard) include investigating labor rates over time, cost trends by WBS, and cost growth. The methodologies presented are consistent with data science best practices – formalized by the popular “grammar of data manipulation” implemented in R through the collection of tidyverse packages. 1 2 The goal is to communicate a flexible, scalable thought process which can be applied to the remainder of the CCDR library, as well as any other dataset. Why are we doing this? The motivation for this paper is a project initiated in fall 2017 by the Project Manager Stryker Brigade Combat Team (PM-SBCT). The cost team had amassed a large collection of CCDRs and was looking for new ways to exploit the data. More specifically, the goal of this project was to develop a Stryker-specific CCDR database and analysis tool to assist in the advanced validation and benchmarking of future data submissions. The CCDR dataset employed for this effort is voluminous and inconsistent. Past CCDR-oriented research typically involved a limited data subset to target a specific, isolated question. This approach resulted in a never ending cycle of data cleansing and rework each and every time. In the interest of future efficiencies, this project operated on the entire Stryker dataset for the purpose of normalizing the data for any/all future analytical purposes. As part of this forward-looking non-recurring effort, the work also established a consistent framework and structure to streamline data maintenance and analysis. While the primary goal of the work described in this paper was analysis of Stryker data, the workflow and data strategies were developed for use with any subset of the CCDR data available to cost analysts today. As such, virtually all the work discussed herein can be easily adapted to another program (or programs). What’s wrong with Excel? Excel is often our tool of choice. Nearly all analysts are trained in its use and it is available on virtually all government workstations. While convenient, Excel has severe limitations. Working with data in it is actually quite difficult. It is very hard to produce repeatable workflows, and it struggles to keep up with even a moderately sized dataset. The software itself enforces no sense of consistency. As a result, Excel analyses tend to be “throw away” – built for a specific task but far too difficult to maintain and expand as a living solution. What are the other options? There are a variety of software tools on the market to work with data. An option growing rapidly in popularity is R, an open source project focused on statistical computing. For decades it lived as a niche product, but has exploded in popularity during the “data science” boom in the mid-2000s. R is able to handle larger sets of data much more easily than Excel. As a scripting language, it is easy to create repeatable, reproducible work. This paper discusses best practices for working with data through the lens of R. However, the concepts are not specific to the software package itself. The topics discussed in this paper can be applied to any other software choice, including Excel. R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/↩ https://www.tidyverse.org/↩ "],
["data.html", "Chapter 2 Working with Data 2.1 Storing and Structuring Data 2.2 Understanding a Grammar", " Chapter 2 Working with Data All data analysis projects follow a series of steps. Practitioners across a wide variety of disciplines may argue the specifics, but a general workflow involves: Research question formulation Data collection Data manipulation and cleaning (wrangling) Exploratory data analysis Formal analysis This workflow can become very cyclical in nature. A specific research question may guide data collection, cleaning, and analysis. If the analysis yields unexpected results, then the process is restarted – hopefully reusing some of the original data. It is very difficult to perfectly execute each step. However, a well-executed data collection followed by proper manipulation and cleaning creates a strong foundation for both exploratory and formal analyses. This foundational dataset should be able to support multiple research questions, including ones of the future. The remainder of Chapter 2 discusses best practices for storing, structuring, and manipulating data. This is not an exhaustive list; it takes experience to fully grasp data manipulation best practices. 2.1 Storing and Structuring Data It all starts with the data structure. More often than not, if you find yourself spending a lot of time reorganizing data before performing a task (e.g., summarizing data, creating a graphic) it is because your data are inefficiently structured. While some post processing will always be required, simple techniques exist to greatly ease data manipulation down the road. Data should be convenient for the computer to work with. This is not necessarily natural and easy for a human to read! This is a critical distinction. Data are stored to be easy to use and manipulate. Data are reported to be easy to consume by a human – be it graphical, tabular, or another form. 2.1.1 Rows and Columns It sounds obvious at first, but structured data are best organized in a tabular format. Think of the table as a collection of rows and columns. Rows represent unique data observations. Each data row must be consistently defined. Deviation from this definition across rows must be avoided. Columns represent variables. Variables are either hard coded data or calculations based on other variables. The consistent definition across all rows within a column allows for the use of vector operations. Vector operations require some practice at first, but are simpler (and more computationally efficient) in the long run. Consider the following data structure in Table 1. This represents a reporting format, not a storage structure. There is white space within the table in the form of blank cells. While easy to read, this creates problems recognizing data entries from a computer’s perspective. This includes both blank rows and spacing in the System column. Column definitions are inconsistent. Sometimes this is unavoidable, but often there are better ways to handle it. In this example we are mixing three ideas. There is cost data (Engine, Remaining, PM), quantity data (Number of Units), and then a calculation (\\(Cost \\times Units\\)) for Total Cost. TABLE1 TABLE2 Table 2 shows a much improved structure. At a quick glance this actually appears more difficult to read. The summary Total Cost is gone, which some may find to be valuable information. The repetition in System adds more text which can be distracting. However, this structure is much more convenient to work with computationally. For larger applications, the structure shown across Table 3 and Table 4 may be even better. Duplication has now been removed. Row and column definitions are clear and consistent. Adding (appending) additional data is easy and the data tables are easily to manipulate to create a more desirable reporting table or graphic. TABLE3 TABLE4 2.1.2 Single Purpose Variables Always create single purpose variables. A prime example of this is within a Work Breakdown Structure (WBS). It is very common to see a WBS with Model incorporated into the structure. In this case, the WBS now represents both a Model and a Cost Element. For example: 1 Surface Vehicle System 1.1 Variant A 1.1.1 Surface Vehicle 1.1.1.1 Engine 1.1.1.2 Remaining Vehicle 1.2 Variant B 1.2.1 Surface Vehicle 1.2.1.1 Engine 1.2.1.2 Remaining Vehicle This structure is useful for reporting, but makes analysis painful. For 1.2.1.1 Engine, we must trace up to its grandparent element, 1.2, to know that it belongs to Variant B. This is not an easy task to do repeatedly within an analysis. A much better structure would be to move the Model out of the WBS into a column tag, as shown in Table 5. TABLE5 This is not intended as an instruction on how to develop a WBS, but instead a demonstration of a preprocessing step to structure the data more effectively for analysis. 2.1.3 Other Data Tips The following sections briefly overview some other topics to consider when structuring data. 2.1.3.1 Variable Names Naming variables can be hard. Names should be descriptive, yet concise. Create variable names that: are unique (i.e., do not name two different columns the same thing) do not have special characters (except the underscore) do not have spaces start with a letter Use underscores and/or CamelCase to string together multiple words. In general, use common sense and include a data dictionary or definition so that there are no questions as to what a specific variable represents. TABLE6 2.1.3.2 Ordered Records Do not ascribe meaning to the physical sequencing of data. Data in a table are unordered. Data rows should be able to be shuffled randomly without any loss of information. Define a variable to store any information that may be required to preserve the ordering of a dataset. Simply sort on that variable should something happen to the data and you need to restore or utilize the ordering. 2.1.3.3 Formatting There is an important distinction between a data value and a data format. The value is the true representation of the data. The formatting is simply what is being displayed. Table 7 shows a few common examples. TABLE7 Best practice is to store unformatted data. Formatting is later applied during reporting. Excel’s usefulness with formatting varies from convenient to highly problematic. Conveniently, Excel allows a cell to store a value, but display formatting. For example, a cell value may be 37284.345, but Excel will display 37,284.3 on the screen. No information is lost, and Excel retains the true cell value for calculations. Problematically, Excel sometimes “guesses” wrong (such as formatting WBS element 1.10 to 1.1) and exports formatted values that are not easily recognized by other software. This is discussed in more detail in the following section. 2.1.3.4 Data Types Be sure to understand your variable data types. There are three generic data types: numeric date text Software developers break these down even further, but less formal applications are fine considering just these three. The following are brief notes on each data type. Numeric data are any type of number; be it a {0, 1} field to represent true and false, an integer, or a number with decimals. A formatted number such as 1,000 is not a numeric field. It looks like a number, but the comma character is text. This is an issue particularly when exporting data as CSV from Excel. Date data are actually a special case of numeric data. Computers store dates as the number of days from some start date (1/1/1900 for Excel). The start date is generally different for each software package. Therefore, the safest way to store a date is as a character string in the ISO date format: YYYY-MM-DD.3 This is an internationally recognized standard that any software will be able to interpret with certainty. Text data are basically anything that is not a number. Any formatting of a numeric variable causes the data to be converted to text. Some software packages will automatically convert back to numeric for you, but most will not. When in doubt, it is best to control data types on your own. In our cost analysis world, WBSs can be highly problematic. A WBS “number” is actually a text field. Most data analysis packages will handle this naturally by defining the entire variable as text. However, we must be careful in Excel. FIGURE1 Figure 1 exemplifies what occurs in Excel when you allow it to handle formatting. Column A shows a WBS number with the Number Format set to Text. Column B contains the same values, but with the General format selected. Excel adheres to standard convention of left justifying text and right justifying numbers, by default. Clearly, this automatic formatting is a problem. Excel recognizes the WBS to be numeric, with the exception of 1.1.1 and 1.1.2, which are treated as text. In particular, 1.10 is now 1.1, as it dropped the trailing zero. This is actually more problematic than it appears. The naive solution is simply to format any WBS column as text. Unfortunately, this fails. A simple SUMIF(), for example, will match 1.1 to both 1.1 and 1.10, summing values incorrectly. The following are some ways to mitigate this issue: Prefix the WBS with a letter (e.g., X1.1, X1.1.1, etc.) to force the structure into text Fill in a complete WBS to the lowest level with zeros (e.g., 1.0.0, 1.1.0, 1.1.1) Parse the WBS into multiple numeric fields (e.g., 1.2.3 becomes three fields: {1, 2, 3}) Do not use Excel 2.1.3.5 Redundant Information Avoid storing redundant data. Redundant data can be calculated from other data fields. Below are a few examples: Only store the child element values of a WBS. The values for the parent tasks can easily be created by summing their children. Do not store subtotals and totals. These can also be calculated very easily. A common occurrence is non-recurring and recurring costs. There is no need to store total cost in this case. Do not store values that can be derived from a formula. There is no need to store density in a dataset that contains mass and volume, assuming the simple equation holds. \\[density = \\frac{mass}{volume}\\] 2.1.3.6 Intermediary Tables Redundant information may actually be important and it is tedious and inefficient to constantly recreate it. Using example (3) from the prior section, your main variable of interest may be density. In this case, create intermediary tables as part of your workflow. FIGURE2 Assuming properly structured data tables, simple transformations (such as those described in the next section) easily and automatically create the intermediary tables. Base your analysis on these representations rather than the raw data for a clean, logical, and maintainable workflow. This strategy naturally segregates data from analysis, making the data more useful for alternative use cases. 2.1.3.7 Sparse Data Data are considered sparse if many of the records have values equal to zero. The representation in Table 8 is considered the dense representation. We can reduce the amount of information stored by assuming that any value not referenced is in fact a zero. TABLE8 The data in Table 9, Table 10, and Table 11 can be used to rebuild that in Table 8. While this does involve an extra step, creating these tables constitutes best practice in a database environment. Under this representation, we only need to store two versus 12 data records. TABLE9 TABLE10 TABLE11 Storing data under a sparse model is not always appropriate. But in certain cases, it yields powerful results by greatly reducing the total volume of data. 2.2 Understanding a Grammar Grammar, the rules to which we abide within a language, enables us to communicate in a structured, repeatable way – even if the meaning itself is nonsensical. While most known in the context of written and spoken language, grammars have become popular elsewhere. Two of the most well-known grammars in data science are the: Grammar of Graphics Grammar of Data Manipulation gram∙mar (noun) The whole system and structure of a language or of languages in general, usually taken as consisting of syntax and morphology (including inflections) and sometimes also phonology and semantics. – OED Online, Oxford University Press These two grammars have been implemented with great success by Hadley Wickham and RStudio in the extremely popular R packages ggplot2 and dplyr.4 The grammars provide a set of instructions, guided by a noun (i.e., a dataset) and a set of verbs. These concepts translate well to other languages and modeling tools, including Excel and Access. Section 4 demonstrates how we leveraged the Grammar of Data Manipulation to create a flexible, scalable framework to analyze Stryker cost data. 2.2.1 Grammar of Data Manipulation https://www.iso.org/iso-8601-date-and-time-format.html↩ Hadley Wickham (https://github.com/hadley) is a Chief Scientist at RStudio, focusing on tool development to “make data science easier, faster, and more fun”. He is viewed as an expert and thought leader in the development of the open source software R, specifically in the areas of data manipulation and visualization.↩ "],
["ccdr.html", "Chapter 3 CCDR Data", " Chapter 3 CCDR Data We describe our methods in this chapter. "],
["stryker.html", "Chapter 4 Stryker Example", " Chapter 4 Stryker Example "],
["closing.html", "Chapter 5 Closing Thoughts", " Chapter 5 Closing Thoughts We have finished a nice book. "],
["references.html", "References", " References "]
]
